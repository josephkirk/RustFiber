High-Performance Fiber-Based Job System Architecture in Rust: Specification and Architecture Guide1. Executive Summary and Architectural ParadigmThe evolution of high-performance computing, particularly within the domain of real-time game engine architecture, has necessitated a paradigm shift from frequency-centric processing to throughput-centric parallelism. As processor clock speeds have plateaued while core counts continue to rise, the traditional single-threaded "main loop" architecture—where simulation, physics, and rendering execute sequentially—has become the primary bottleneck in engine performance. The industry response, crystallized by Naughty Dog’s pivotal development for The Last of Us Remastered on the PlayStation 4, is the fiber-based job system. This architecture decomposes monolithic frame logic into granular, dependency-aware units of work ("jobs") that are multiplexed onto a fixed pool of hardware threads via lightweight, user-space execution contexts known as fibers.1This report provides a comprehensive architectural specification and implementation guide for constructing a robust, industrial-grade fiber job system using the Rust programming language. While the original implementations of this architecture were predominantly in C++, Rust offers a distinct set of advantages and challenges. Its strict ownership model, compile-time thread-safety guarantees (Send and Sync), and zero-cost abstractions allow for a system that eliminates entire classes of concurrency hazards—such as data races and dangling pointers—without sacrificing the low-level control required for sub-millisecond frame times. However, the implementation of such a system in Rust requires navigating the tension between its safety guarantees and the inherently unsafe nature of context switching, intrusive data structures, and raw memory manipulation required for maximum throughput.3The proposed architecture adopts a M:N threading model, scheduling M logical fibers onto N OS-level worker threads. This ensures that the number of active OS threads never exceeds the number of available physical cores, eliminating the cost of kernel-level context switching and scheduler preemption. Synchronization is achieved not through blocking OS primitives, which induce latency, but through atomic counters and a cooperative fiber yielding mechanism. When a job encounters a dependency, its fiber is suspended, and the worker thread immediately switches to another ready fiber, maintaining near-100% CPU utilization.21.1 The Throughput ImperativeIn real-time rendering, the primary performance metric is frame latency. A fiber-based job system reduces latency by widening the execution pipeline. By breaking sequential dependencies and enabling fine-grained parallelism, the engine can saturate all available execution units. The philosophy defined in this specification prioritizes three pillars:Zero OS Interference: Worker threads are pinned to cores and never yield to the OS kernel if work is available. All suspension is handled in user space.Data Locality: Jobs are executed in a Last-In, First-Out (LIFO) manner on local queues to maximize cache coherency.Lock-Free Synchronization: The hot path of job scheduling and dependency resolution relies exclusively on atomic operations and lock-free data structures, avoiding heavy mutex contention.62. Theoretical Foundations and Core ConceptsTo construct a robust job system, one must understand the theoretical primitives that replace standard operating system facilities. The architecture effectively implements a cooperative multitasking operating system within the runtime of the application, managing its own scheduling, memory, and synchronization.2.1 The Fiber: User-Space Context SwitchingA fiber is the fundamental unit of execution in this architecture. Unlike an operating system thread, which is managed by the kernel and incurs significant overhead during context switching (typically 1-2 microseconds involving mode transitions and cache flushing), a fiber is a purely user-space construct. It consists of a stack and a set of CPU registers (instruction pointer, stack pointer, and callee-saved registers). Switching between fibers is a manual operation performed by the application, typically taking only 10-50 nanoseconds.5In the context of Rust, it is crucial to distinguish between stackless coroutines (the mechanism behind async/await) and stackful coroutines (fibers).Stackless Coroutines (Async/Await): These are state machines generated by the compiler. They are extremely memory efficient but suffer from "function coloring"—an async function can only be awaited by another async function. This makes integrating them into legacy systems or complex call graphs difficult.9Stackful Coroutines (Fibers): These possess a true stack. This allows for "deep yielding," where a function nested twenty calls deep in the stack can request a yield (e.g., wait_for_counter), and the entire call stack is preserved while the processor switches tasks. This property is essential for game logic, which often follows an imperative flow (e.g., "Move Character" -> "Check Collision" -> "Wait for Physics Result") that is awkward to express as a state machine.12.2 The Job: Granularity and LifecycleA job is a transient unit of work, conceptually represented as a closure or function pointer paired with a data payload. In Rust terms, this is often modeled as Box<dyn FnOnce() + Send>. However, given the high frequency of job creation—potentially tens of thousands per frame—heap allocation for every job is prohibitive. The architecture necessitates a Small Job Optimization, where job structures are kept small (e.g., 64-128 bytes) and stored in contiguous memory buffers to preserve cache locality.4The lifecycle of a job involves:Creation: The job is instantiated with its data and dependency counter.Submission: The job is pushed to a work queue (local or global).Execution: A worker thread acquires the job, associates it with a free fiber, and executes the code.Completion: Upon return, the fiber is recycled, and the job's associated counter is decremented, potentially waking dependent jobs.52.3 The Atomic Counter: The Dependency GraphThe universal synchronization primitive in this architecture is the Atomic Counter. Unlike a binary semaphore or a mutex, the atomic counter allows for complex, one-to-many dependency resolution. It effectively models a dynamic Directed Acyclic Graph (DAG) of execution.Wait: A job can wait for a counter to reach zero. If the counter is non-zero, the job's fiber is suspended.Signal: When a job completes, it decrements a counter. If the counter transitions to zero, all fibers waiting on that counter are resumed.This mechanism decouples the "producer" of a dependency from the "consumer," allowing the scheduler to fill the intervening time with unrelated work.62.4 Comparative Analysis of Concurrency ModelsFeatureOS ThreadsAsync/Await (Futures)Fiber Job SystemContext Switch CostHigh (Kernel Mode, >1000ns)Low (State Machine step)Very Low (Register Swap, ~50ns)Memory OverheadHigh (MBs per thread)Very Low (Bytes per future)Medium (KBs per stack)SchedulingPreemptive (OS Kernel)Cooperative (Poll-based)Cooperative (User Scheduler)Blocking BehaviorBlocks CoreReturns PendingSwaps Stack (Core remains active)Call Stack DepthDeepShallow (State Machine)Deep (True Stack)Rust SafetyStandard std::threadCompiler VerifiedRequires unsafe encapsulationThis table highlights why fibers are the preferred choice for CPU-bound tasks in game engines: they combine the deep call stack capability of threads with the low switching cost of futures.53. System Architecture SpecificationThe proposed system is composed of four primary subsystems: the Worker Thread Pool, the Work-Stealing Queues, the Fiber Pool, and the Wait List mechanism.3.1 Worker Thread Topology and AffinityThe system initializes N worker threads, where N typically corresponds to the number of logical cores available on the host machine. To maximize performance, it is imperative to enforce CPU Affinity (pinning).Mechanism: Each worker thread is pinned to a specific logical core index using the core_affinity crate.Rationale: Modern CPUs rely heavily on L1 and L2 caches. If the OS scheduler migrates a thread from Core 0 to Core 1, the thread loses its hot cache state, resulting in a flurry of cache misses. Pinning ensures that the thread's working set remains local to the core.2Main Thread Integration: The main application thread is treated as one of the workers (typically Worker 0). This prevents the main thread from idling while waiting for the frame to finish; instead, it actively participates in executing the job graph.43.2 The Lock-Free Work-Stealing QueueTo distribute load dynamically without a central bottleneck, the system utilizes Work Stealing. The standard implementation is the Chase-Lev Deque (Double-Ended Queue), which is robustly implemented in the Rust ecosystem by the crossbeam-deque crate.3.2.1 Queue Structure and OperationsEach worker thread owns a local deque. The operations are defined to optimize cache behavior:Push (Bottom): The owner thread adds new jobs to the bottom of the deque.Pop (Bottom): The owner thread takes jobs from the bottom. This LIFO (Last-In, First-Out) behavior is critical. A newly created job likely operates on data that is still hot in the CPU cache (e.g., a parent job processes an entity and spawns a sub-job to process the entity's physics). Processing the child immediately preserves this locality.13Steal (Top): When a worker runs out of local jobs, it targets another worker's deque and steals from the top (FIFO). Stealing the oldest job is advantageous because, in a recursive decomposition, the oldest jobs are typically the largest "root" tasks that will generate significant sub-work, thereby keeping the thief busy for longer and reducing the frequency of steal attempts.143.2.2 The Injector QueueIn addition to local queues, a global Injector Queue is maintained for jobs originating from threads outside the job system (e.g., network threads, OS callbacks). Workers check the injector periodically (e.g., every 100 iterations of the loop) to ensure external work is eventually processed.133.3 The Fiber Pool and Stack ManagementAllocating stacks is an expensive operation involving system calls (mmap or VirtualAlloc). Therefore, stacks must be pre-allocated and pooled.Sizing: A pool of fibers is allocated at startup. The size of each stack is a tunable parameter, typically 64KB or 128KB for game logic.Recycling: When a job completes, its fiber is not deallocated. The stack pointer is reset, and the fiber is marked as "free" in the pool, ready to be reused by the next incoming job.Implementation: The corosensei crate provides a safe and efficient abstraction for stackful coroutines in Rust, handling the architecture-specific assembly for context switching and ensuring compatibility with Rust's unwinding mechanism.173.4 The Wait List (Intrusive Linked List)The most complex component is the mechanism for putting fibers to sleep. When wait_for_counter is called, the fiber cannot be simply placed in a Vec<Fiber>, as this would require heap allocation in the critical path.Intrusive Structure: The solution is an intrusive linked list. The Fiber structure itself contains next and prev pointers.Mechanism: When a fiber waits, it links itself into the atomic counter's wait list. Because the fiber is suspended, its stack (and the node representing it) remains valid in memory. The atomic counter holds the head of this list.Rust Challenges: Rust's ownership model makes intrusive lists difficult, as a struct cannot easily own a reference to itself. Implementation requires careful use of unsafe pointers or the Pin wrapper to guarantee that the fiber structure does not move in memory while it is linked in a wait list.194. Rust Implementation SpecificationThis section details the specific Rust structures, traits, and safety considerations required to implement the architecture.4.1 Structural DefinitionsThe core of the system is the JobSystem struct, which is shared across all worker threads via an Arc.Rustpub struct JobSystem {
    // Global queue for external jobs
    injector: crossbeam_deque::Injector<Job>,
    // Stealers for all worker threads (allows stealing)
    stealers: Vec<crossbeam_deque::Stealer<Job>>,
    // The pool of reusable fibers
    fiber_pool: FiberPool,
    // Atomic counters pool (to avoid allocs)
    counter_pool: CounterPool,
}

pub struct Worker {
    // Local queue for this thread (LIFO access)
    local_queue: crossbeam_deque::Worker<Job>,
    // Reference to the global system
    system: Arc<JobSystem>,
    // The currently executing fiber (if any)
    current_fiber: Option<FiberHandle>,
}
4.2 The Job Type and Send SafetyA job is defined as a closure. Crucially, because a job may start on Thread A, yield, and resume on Thread B, the closure and all its captures must be Send.Rust// The fundamental unit of work
pub type Job = Box<dyn FnOnce() + Send>;
Constraint: This Send requirement implies that jobs cannot capture thread-local data (like RefCell or standard std::thread_local!) unless that data is strictly scoped to the synchronous execution of the job and dropped before yielding. The compiler will enforce this, preventing a class of bugs where thread-local state is accidentally accessed from the wrong thread after migration.224.3 Implementing the Atomic CounterThe AtomicCounter is the synchronization hub. To avoid false sharing (cache line contention), the counter struct must be padded to the size of a cache line (typically 64 or 128 bytes).Rustuse std::sync::atomic::{AtomicUsize, Ordering};
use crossbeam_utils::CachePadded;

#[repr(C)]
pub struct AtomicCounter {
    // The count of dependencies
    value: CachePadded<AtomicUsize>,
    // The list of fibers waiting for this counter to reach 0
    waiting_fibers: Mutex<IntrusiveList<Fiber>>,
}
Note: While Mutex is used here for clarity, a high-performance implementation would use a lock-free intrusive stack driven by compare_and_swap operations on an AtomicPtr head node.4.3.1 Memory OrderingCorrect memory ordering is vital.Increment (fetch_add): Can often be Ordering::Relaxed if the logic only depends on the value eventually reaching zero.Decrement (fetch_sub): Must usually be Ordering::Release to ensure that all memory writes performed by the job are visible to the waiting job that will be resumed.Check (load): When checking if the counter is zero, Ordering::Acquire is required to ensure that the resuming fiber sees the data written by the prerequisite jobs.74.4 The Fiber Switch MechanismThe corosensei crate is used to implement the switch. The worker thread runs a "Main Loop" which acts as the scheduler.Scheduler Loop Logic:Rustfn worker_loop(worker: &mut Worker) {
    loop {
        // 1. Try to pop from local queue
        let job = worker.local_queue.pop()
            // 2. Try to pop from injector
           .or_else(|| worker.system.injector.steal().success())
            // 3. Try to steal from neighbors
           .or_else(|| steal_from_others(&worker.system.stealers));

        if let Some(job) = job {
            // Acquire a fiber from the pool
            let mut fiber = worker.system.fiber_pool.get();
            
            // Switch Context: Run the job inside the fiber
            let result = fiber.run(move |

| {
                (job)();
            });

            // Handle the result of the switch
            match result {
                FiberState::Complete => {
                    // Job finished normally. Recycle fiber.
                    worker.system.fiber_pool.return(fiber);
                }
                FiberState::Yielded(wait_node) => {
                    // Job called wait_for_counter. 
                    // The fiber is now linked in the counter's wait list.
                    // Do NOT return to pool. It is effectively "owned" by the counter.
                }
                FiberState::Panicked(payload) => {
                    // Critical error handling (see Section 6)
                }
            }
        } else {
            // No work found. Park thread or yield CPU.
            std::thread::yield_now();
        }
    }
}
This logic demonstrates the Scheduler-Fiber separation. The scheduler (worker loop) is not a fiber; it is the host environment. It switches into fibers to run jobs. When a fiber yields, it switches back to the scheduler, which then finds the next task.55. Advanced Implementation Challenges5.1 Memory Management and Bump AllocationHeap allocation (Box::new, Vec::push) inside a job is a performance killer due to lock contention in the global allocator.Solution: Thread-Local Bump Allocation.Each worker thread maintains a bumpalo::Bump arena. Jobs can request a slice of memory from this arena.Constraint: Because jobs can migrate between threads, a pointer allocated in Thread A's bump arena is unsafe if the job moves to Thread B and Thread A resets its arena.Architecture: To solve this, the system uses Frame Linear Allocators. Memory allocated from the frame allocator is guaranteed to persist until the entire frame is completed and the system synchronizes. Only then are the allocators reset. This allows jobs to pass pointers (e.g., to an array of results) safely between them, regardless of which thread executes them.265.2 False Sharing and Cache AlignmentFalse sharing occurs when two independent atomic variables (e.g., two different counters) reside on the same cache line. Thread A writing to Counter 1 invalidates the cache line for Thread B reading Counter 2, causing a "ping-pong" effect that serializes execution at the hardware level.Impact: This can degrade performance by orders of magnitude.Mitigation: The Job struct and AtomicCounter struct must be annotated with #[repr(align(64))] (or 128 for generic compatibility). This forces the compiler to place each instance on a fresh cache line. Rust's crossbeam_utils::CachePadded wrapper is the standard tool for this.295.3 Panic Handling and UnwindingIn Rust, a panic initiates stack unwinding. If a panic occurs inside a fiber and unwinds past the fiber's stack boundary into the scheduler's stack (across the FFI boundary of the context switch), it leads to Undefined Behavior (UB) and often a hard crash.Requirement: All job execution must be wrapped in std::panic::catch_unwind.Recovery Strategy: If a job panics, the system must catch it. Crucially, if the job held a reference to a counter it was supposed to decrement, that counter will now hang forever, deadlocking the system.RAII Guards: The implementation should use a RAII (Resource Acquisition Is Initialization) Guard for the counter decrement. The guard's Drop implementation ensures the counter is decremented whether the scope exits normally or via stack unwinding. This ensures the dependency graph resolves even in the presence of logic errors.316. Implementation Guide: Step-by-Step6.1 Step 1: Defining the Fiber StackUsing corosensei, we define the stack size and handling.Rustuse corosensei::{Coroutine, CoroutineResult};

struct Fiber {
    coroutine: Coroutine<Job, (), ()>,
}

impl Fiber {
    fn new(stack_size: usize) -> Self {
        let coroutine = Coroutine::new(move |yielder, job: Job| {
            // Execute the job
            (job)();
            // Loop or return to pool logic handled by scheduler
        });
        Fiber { coroutine }
    }
}
6.2 Step 2: The wait_for_counter ImplementationThis function demonstrates the interaction between the high-level logic and the low-level fiber switch.Rustpub fn wait_for_counter(counter: &AtomicCounter, scheduler: &Scheduler) {
    // Optimization: Fast path if already zero
    if counter.value.load(Ordering::Acquire) == 0 {
        return;
    }

    // Capture the current fiber's handle
    let fiber_handle = scheduler.current_fiber().expect("Not in a fiber!");

    // Link into the wait list (Pseudo-code for intrusive insert)
    counter.wait_list.lock().push(fiber_handle);

    // Yield execution
    // This suspends the stack and returns control to the worker loop
    scheduler.yield_fiber(); 
}
6.3 Step 3: Bootstrapping the SystemThe initialization phase must spawn the threads and pin them.Rustpub fn initialize(num_threads: usize) -> Arc<JobSystem> {
    let system = Arc::new(JobSystem::new(num_threads));
    let core_ids = core_affinity::get_core_ids().unwrap();

    for (i, core_id) in core_ids.into_iter().take(num_threads).enumerate() {
        let sys = system.clone();
        std::thread::spawn(move |

| {
            // Pin thread to core
            core_affinity::set_for_current(core_id);
            // Enter the scheduler loop
            worker_loop(i, sys);
        });
    }
    system
}
7. Performance Tuning and OptimizationTo achieve the robustness required by the specification, several tuning parameters and behaviors must be addressed.7.1 Thundering Herd MitigationWhen a counter reaches zero, it might wake a large number of waiting jobs (e.g., "Physics Update" finishes, waking 500 "Entity Logic" jobs). If the decrementing thread tries to push all 500 fibers to the global queue individually, it creates massive contention on the queue lock.Solution: Implement Batch Waking. The wait_list should be drained into a temporary list, and then pushed to the global injector queue (or the local queue) as a single batch operation. crossbeam-deque supports batch stealing and pushing, which amortizes the synchronization cost over the entire batch.137.2 Backoff StrategiesWhen a worker thread finds no work (local queue empty, global queue empty, stealing failed), it must not busy-spin at 100% CPU usage, as this generates heat and consumes power (relevant for mobile/consoles).Strategy: Implement an exponential backoff.Spin-loop hint (std::hint::spin_loop()) for a few iterations.std::thread::yield_now() to allow the OS to schedule hyper-threads.std::thread::park() or CondVar::wait() for deep sleep if the system is truly idle. Note that waking from park is high-latency, so this should only be used if idle for a significant duration (e.g., >1ms).337.3 Debugging and ProfilingDebugging fiber systems is notoriously difficult because standard debuggers (GDB/LLDB) often get confused by the manual stack switching.Instrumentation: Integrate tracing (the Rust crate) to emit events at job start, job end, and wait start/end.Visualizer: A robust system should implement a visualizer output (e.g., Chrome Tracing format JSON) to view the "job graph" timeline. This allows developers to see gaps in execution (bubbles) where cores are idle due to dependency stalls.358. ConclusionThe architecture detailed in this report represents a synthesis of the high-performance techniques pioneered by Naughty Dog and the modern safety guarantees provided by Rust. By moving context switching to user space via fibers, the system decouples logical concurrency from physical parallelism, allowing the engine to scale linearly with the number of available cores.While the implementation demands a rigorous approach to unsafe code—particularly regarding memory allocation, intrusive data structures, and context switching—the result is a job system that offers:Safety: Compile-time verification of thread-safety for job data via Send/Sync.Performance: Microsecond-level job dispatch and sub-microsecond synchronization.Scalability: A lock-free design that minimizes contention as core counts increase.For professional game engine development in Rust, this fiber-based architecture is not merely an optimization; it is the foundational requirement for next-generation performance.(End of Report)